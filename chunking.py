import requests
import json
import pdfplumber
import re
import os
from dotenv import load_dotenv

load_dotenv()
API_KEY = os.getenv("API_KEY")
API_URL = "https://api-gateway.netdb.csie.ncku.edu.tw/api/generate"

CHUNKED_LIST_FILE = "chunked_pdfs.json"
OUTPUT_FILE = "all_chunks.json"
PDF_DIR = "./pdf"

# ========================
# Load processed pdf list
# ========================
if os.path.exists(CHUNKED_LIST_FILE):
    with open(CHUNKED_LIST_FILE, "r", encoding="utf-8") as f:
        chunked_pdfs = set(json.load(f))
else:
    chunked_pdfs = set()

# ========================
# Scan PDF directory
# ========================
pdf_files = [f for f in os.listdir(PDF_DIR) if f.lower().endswith(".pdf")]

print(f"ğŸ“„ ç™¼ç¾ {len(pdf_files)} ä»½ PDF")

def extract_text_by_page(pdf_path):
    """å›å‚³æ¯é æ–‡å­—åˆ—è¡¨"""
    pages_text = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text:
                # ç§»é™¤ä¸­æ–‡èˆ‡æ¨™é»
                clean_text = re.sub(
                    r"[\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿã€ã€‘ï¼ˆï¼‰ã€Šã€‹ï¼›ï¼š]", "", text
                )
                pages_text.append(clean_text)
    return pages_text

def extract_json_objects_from_text(text):
    objects = []
    stack = []
    start = None

    for i, ch in enumerate(text):
        if ch == "{":
            if not stack:
                start = i
            stack.append("{")
        elif ch == "}":
            if stack:
                stack.pop()
                if not stack and start is not None:
                    obj_text = text[start : i + 1]
                    objects.append(obj_text)
                    start = None
    return objects

def generate_prompt(content, difficulty_level):
    return f"""
ä½ æ˜¯ä¸€å€‹è‹±æ–‡æ•™å­¸å°ˆå®¶ï¼Œä¸¦è² è²¬é¡Œç›®åˆ‡åˆ†èˆ‡çµæ§‹åŒ–æ¨™è¨»ï¼ˆchunkingï¼‰ç³»çµ±ã€‚

ã€ä»»å‹™ç›®æ¨™ã€‘
è«‹å°‡ä»¥ä¸‹è‹±æ–‡è€ƒå·å…§å®¹åˆ‡åˆ†ç‚ºã€Œå¯ç›´æ¥ç”¨æ–¼å‡ºé¡Œèˆ‡ RAG æª¢ç´¢ã€çš„ chunkã€‚

ã€é‡è¦åŸå‰‡ã€‘
1. ä¸€å€‹ chunk å¿…é ˆæ˜¯ã€Œå®Œæ•´ã€ä¸å¯æ‹†åˆ†ã€çš„é¡Œå–®ä½ã€‚
2. ä¸å¯å°‡åŒä¸€é¡Œçš„é¡Œå¹¹èˆ‡é¸é …æ‹†åˆ°ä¸åŒ chunkã€‚
3. å…‹æ¼å­—èˆ‡é–±è®€ç†è§£é¡Œï¼Œæ•´é¡Œï¼ˆæ–‡ç«  + é¡Œç›® + é¸é …ï¼‰æ”¾åœ¨åŒä¸€å€‹ chunkã€‚
4. ä¿æŒé¡Œç›®åŸå§‹é †åºã€‚
5. ä¸€å®šè¦æ¨™è¨»æ–‡æ³•, ä¸¦ä¸”è¶Šè©³ç´°è¶Šå¥½ï¼Œæœ‰æ¶‰åŠåˆ°çš„æ–‡æ³•å…¨éƒ¨æ”¾é€²ä¾†ï¼Œçµ•å°ä¸å¯ä»¥æ˜¯ empty list
6. è®€å–åˆ°çš„æ–‡å­—å¯èƒ½æ˜¯ç ´ç¢çš„ã€é †åºä¸å°çš„ï¼Œè«‹å‹™å¿…æŠŠç›¸é—œçš„é¡Œç›®ã€é¸é …æ­¸é¡åœ¨ä¸€èµ·ï¼Œå¦‚æœç„¡æ³•æ­¸é¡ï¼Œå‰‡ç›´æ¥æ‹‹æ£„ï¼Œç¦æ­¢éš¨æ„æ£æ¸¬ã€è®Šé€ é¡Œç›®
7. è½åŠ›é¡Œç›®(å³æ²’æœ‰é¡Œå¹¹ï¼Œåªæœ‰é¸é …çš„é¡Œç›®)ï¼Œè«‹ç›´æ¥è·³é
8. åš´æ ¼å€åˆ† chunk é¡å‹ï¼Œå…ˆæª¢æŸ¥æ˜¯ä¸æ˜¯ cloze æˆ–è€… readingï¼Œå¦‚æœæ˜¯ï¼ŒæŠŠæ•´ç¯‡æ–‡ç«  + æ‰€æœ‰å°æ‡‰é¡Œç›®æ”¾åœ¨åŒä¸€å€‹ chunk
9. è«‹ä¸è¦ç”Ÿæˆä»»ä½•ä¸åŒ…å« passage çš„ reading_questionï¼Œå¦å‰‡å­¸ç”Ÿå°‡ç„¡æ³•åˆ¤æ–·ç­”æ¡ˆ
10. åŒç¬¬ 9 é»ï¼Œ cloze_question è¦å¿…é ˆåŒ…å« passageï¼Œå¦å‰‡è«‹ä¸è¦ç”Ÿæˆ
11. æª¢æŸ¥ chunk_text ä¸­çš„è³‡è¨Šæ˜¯å¦è¶³ä»¥é¸å‡ºç­”æ¡ˆï¼Œå¦‚æœä¸è¡Œï¼Œè«‹åˆªé™¤æ­¤ chunk
12. æª¢æŸ¥ chunk_text ä¸­çš„è³‡è¨Šæ˜¯å¦è¶³ä»¥é¸å‡ºç­”æ¡ˆï¼Œå¦‚æœä¸è¡Œï¼Œè«‹åˆªé™¤æ­¤ chunk
13. æª¢æŸ¥ chunk_text ä¸­çš„è³‡è¨Šæ˜¯å¦è¶³ä»¥é¸å‡ºç­”æ¡ˆï¼Œå¦‚æœä¸è¡Œï¼Œè«‹åˆªé™¤æ­¤ chunk

ã€Chunk é¡å‹ã€‘ 
- single_question
- cloze_question  # æ•´é¡Œæ–‡ç«  + æ‰€æœ‰å°æ‡‰é¡Œç›®
- reading_question  # æ•´é¡Œæ–‡ç«  + æ‰€æœ‰å°æ‡‰é¡Œç›®

ã€æ¯å€‹ chunk æ¬„ä½ï¼Œä¸å¯ä»¥æœ‰ä»»ä½•çœç•¥,ä¹Ÿä¸å¯ä»¥ä»»æ„æ·»åŠ ã€‘
- chunk_type
- levelï¼š{difficulty_level}
- grammar_pointsï¼ˆè«‹æ ¹æ“šæä¾›çš„æ–‡æ³•æ¸…å–®åˆ¤æ–·ï¼‰
- chunk_textï¼šå®Œæ•´é¡Œå¹¹ + é¸é … + æ–‡ç« ï¼ˆå¦‚æœ‰ï¼‰
- filename: {pdf_path}

ã€è¼¸å‡ºè¦å‰‡ã€‘
- åš´æ ¼è¼¸å‡º JSON array
- æ¯å€‹æ¬„ä½éƒ½å¿…é ˆå­˜åœ¨ï¼Œç©ºå€¼å¡« null æˆ–ç©º list
- ä¸å›ç­”é¡Œç›®ã€ä¸æä¾›è§£æ
- ä¸è¼¸å‡ºèªªæ˜æ–‡å­—æˆ–å…¶ä»–æ–‡å­—

ã€grammar_points listã€‘
ä¸€ã€åè©èˆ‡å† è©
- å°ˆæœ‰åè©
- å¯æ•¸åè©èˆ‡ä¸å¯æ•¸åè©
- åè©å–®è¤‡æ•¸
- ä¸å®šå† è© a / an
- å®šå† è© the
- æ‰€æœ‰æ ¼ï¼ˆâ€™s / ofï¼‰
- one / ones
- ä¸å®šæ•¸é‡è©ï¼ˆsome, any, many, much, a few, a little, a lot of, lots ofï¼‰

äºŒã€ä»£åè©
- äººç¨±ä»£åè©ä¸»æ ¼
- äººç¨±ä»£åè©å—æ ¼
- æ‰€æœ‰ä»£åè©ï¼ˆå½¢å®¹è©æ€§ / åè©æ€§ï¼‰
- äººç¨±ä»£åè©è¤‡æ•¸
- åèº«ä»£åè©
- ä¸å®šä»£åè©
- whose çš„ç”¨æ³•

ä¸‰ã€æŒ‡ç¤ºèˆ‡å¼•ä»‹çµæ§‹
- this / that
- these / those
- there is / there are

å››ã€å‹•è©åŸºæœ¬å¥å‹
- be å‹•è©ç¾åœ¨å¼
- be å‹•è©éå»å¼
- ä¸€èˆ¬å‹•è©ç¾åœ¨ç°¡å–®å¼
- ç¬¬ä¸‰äººç¨±å–®æ•¸ç¾åœ¨å¼
- åŠ©å‹•è© do / does / did
- ç¥ˆä½¿å¥

äº”ã€æ™‚æ…‹
- ç¾åœ¨é€²è¡Œå¼
- éå»ç°¡å–®å¼ï¼ˆè¦å‰‡ / ä¸è¦å‰‡ï¼‰
- éå»é€²è¡Œå¼
- æœªä¾†å¼ï¼ˆwill / be going toï¼‰
- ç¾åœ¨å®Œæˆå¼
- å‹•è©æ™‚æ…‹ç¶œåˆæ‡‰ç”¨

å…­ã€åŠ©å‹•è©èˆ‡æƒ…æ…‹å‹•è©
- can
- have to / has to
- should / could / would
- used to

ä¸ƒã€ç–‘å•å¥
- Yes / No å•å¥
- Wh- å•å¥
- which çš„ç”¨æ³•
- å•å¹´ç´€
- å•æ™‚é–“
- é–“æ¥å•å¥
- é™„åŠ å•å¥

å…«ã€é€£æ¥è©èˆ‡å¾å±¬çµæ§‹
- and / but
- because / if
- when / before / after
- while / as
- though
- not onlyâ€¦but alsoâ€¦
- no matter + wh-

ä¹ã€ä»‹ç³»è©èˆ‡å‰¯è©
- åœ°é»ä»‹ç³»è©
- æ™‚é–“ä»‹ç³»è©
- äº¤é€šå·¥å…·ä»‹ç³»è©
- ä»‹ç³»è©ç‰‡èªç•¶å½¢å®¹è©
- å‰¯è©ï¼ˆæ–¹å¼ / åœ°é» / æ™‚é–“ / é »ç‡ï¼‰
- é »ç‡å‰¯è©

åã€å½¢å®¹è©èˆ‡æ¯”è¼ƒ
- å½¢å®¹è©ç”¨æ³•
- å½¢å®¹è©æ¯”è¼ƒç´š
- å½¢å®¹è©æœ€é«˜ç´š
- å‰¯è©æ¯”è¼ƒ

åä¸€ã€ç‰¹æ®Šå‹•è©
- æ„Ÿå®˜å‹•è©
- é€£ç¶´å‹•è©
- æˆèˆ‡å‹•è©
- ä½¿å½¹å‹•è©
- spend / cost / take / pay
- stop / remember / forget

åäºŒã€éé™å®šå‹•è©
- å‹•åè©
- ä¸å®šè©
- tooâ€¦toâ€¦
- soâ€¦thatâ€¦

åä¸‰ã€åˆ†è©èˆ‡èªæ…‹
- ç¾åœ¨åˆ†è© / éå»åˆ†è©
- æƒ…ç·’å‹•è©èˆ‡æƒ…ç·’å½¢å®¹è©
- è¢«å‹•èªæ…‹

åå››ã€åè©å­å¥èˆ‡é—œä¿‚å­å¥
- that + åè©å­å¥
- åè©å­å¥ï¼ˆwhat / whether / ifï¼‰
- è™›ä¸»è© it
- é—œä¿‚ä»£åè©

åäº”ã€å…¶ä»–
- æ„Ÿå˜†å¥
- è‹±æ–‡ä¸­ã€Œä¹Ÿã€çš„ç”¨æ³•
- it ç”¨æ³•ç¸½æ•´ç†
- æ›¸ä¿¡æ ¼å¼

ã€æ•™æå…§å®¹å¦‚ä¸‹ã€‘
\"\"\" {content} \"\"\"
"""

# --- ä¸»æµç¨‹ ---
for pdf_file in pdf_files:
    if pdf_file in chunked_pdfs:
        print(f"â© è·³éå·²è™•ç†ï¼š{pdf_file}")
        continue

    pdf_path = os.path.join(PDF_DIR, pdf_file)
    print(f"\nğŸš€ é–‹å§‹è™•ç†ï¼š{pdf_file}")

    pages_text = extract_text_by_page(pdf_path)

    all_responses = []

    if "åœ‹ä¸­" in pdf_path:
        difficulty_level = "junior_high"
    elif "é«˜ä¸­" in pdf_path:
        difficulty_level = "senior_high"
    else:
        difficulty_level = "junior_high"

    for page_num, page_text in enumerate(pages_text, start=1):
        print(
            f"\n=== è™•ç†æ–‡ä»¶ {pdf_file} ç¬¬ {page_num} é  é›£åº¦ {difficulty_level}===\n"
        )
        prompt = generate_prompt(page_text, difficulty_level)
        payload = {"model": "gpt-oss:120b", "prompt": prompt, "stream": True}

        page_response = ""
        with requests.post(
            API_URL,
            headers={
                "Authorization": f"Bearer {API_KEY}",
                "Content-Type": "application/json",
            },
            json=payload,
            stream=True,
        ) as r:
            r.raise_for_status()
            for line in r.iter_lines():
                if line:
                    token_json = json.loads(line.decode())
                    token_text = token_json.get("response", "")
                    page_response += token_text
                    print(token_text, end="", flush=True)

        all_responses.append(page_response)

    print("\n\n--- å®Œæ•´ JSON çµæœ ---")
    # å°‡æ¯é çµæœåˆä½µæˆä¸€å€‹ JSON array
    combined_json = "[{}]".format(",".join([r.strip("[]") for r in all_responses]))
    print(combined_json)

    with open("output.json", "w", encoding="utf-8") as f:
        f.write(combined_json)

    # ===== Flatten JSON =====
    with open("output.json", "r", encoding="utf-8") as f:
        raw_text = f.read()

    objects_text = extract_json_objects_from_text(raw_text)

    print(f"æŠ½å‡º {len(objects_text)} å€‹ {{}}")

    # å˜—è©¦è½‰æˆçœŸæ­£çš„ JSON

    valid_objects = []
    for obj in objects_text:
        try:
            valid_objects.append(json.loads(obj))
        except json.JSONDecodeError:
            pass  # æœ‰å£çš„å°±è·³é

    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            last_chunk = json.load(f)
    else:
        last_chunk = []

    last_chunk.extend(valid_objects)
    # print(type(last_chunk))

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(last_chunk, f, ensure_ascii=False, indent=2)

    chunked_pdfs.add(pdf_file)
    with open(CHUNKED_LIST_FILE, "w", encoding="utf-8") as f:
        json.dump(sorted(chunked_pdfs), f, ensure_ascii=False, indent=2)
